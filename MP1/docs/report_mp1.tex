% setup the document and include packages
\documentclass{article}[12pt]
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{cancel}
\usepackage{ntheorem}
\usepackage{algorithm2e}
\usepackage{caption}

\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother

% define Continue for algorithms
\SetKw{Continue}{continue}

% redefine QED symbol
\renewcommand{\qedsymbol}{\rule{0.7em}{0.7em}}

% define lemma and result theorem-styled sections
\newtheorem{lemma}{Lemma}[section]
\newtheorem{result}{Result}[section]

% Don't print the semicolon in algorithms
\DontPrintSemicolon

% define the title that will be used in the report
\title{CS 440 MP1 \\ Section Q4}
\author{
Christian Howard \\ howard28@illinois.edu
\and
Luke Pitstick \\ pitstck2@illinois.edu
\and
Liuyi Shi \\ liuyis2@illinois.edu
}
\date{} % don't set a date because we don't need this shit


% start the document
\begin{document}
   
   % create the title page 
   \maketitle
   \begin{abstract}
   Within this report, we investigate creating agents that use different path planning algorithms and do an analysis of how they compare for a set of sample problems. \textbf{Insert summary of results}. We later apply $A^*$ algorithms to guide an agent to solving the  Sokoban puzzle. \textbf{Insert summary of results}.
   \end{abstract}
   \newpage
   
   % create table of contents on separate page
   \tableofcontents
   \newpage
   
   % start section covering work on Part 1.1
   \section{Part 1 - Multi-Goal Search}
   Within this section, we investigate using search algorithms to tackle single-goal and multi-goal objectives. To handle this problem, the provided maze is processed into a graph, $G_m$, that can be used to figure out what actions a given agent can take and when they reach some goal node. To model the motion of some agent, we define the state, $\boldsymbol{x}$, as the following:  
   
   \begin{align*}
   \boldsymbol{x} = \left( p, b_1, b_2, \cdots, b_n \right)
   \end{align*}
   
   where $p$ is an index representing the graph node the agent is on and $b_k$ represents a Boolean variable that is $1$ (true) when the agent has passed through the $k$\textsuperscript{th} goal point at some point prior and $0$ (false) otherwise. Note that this state is always implicitly dependent on the maze graph, $G_m$. We know an agent has completed a maze when it reaches a state that satisfies the following:
   
   \begin{align*}
   b_1 \wedge b_2 \wedge \cdots \wedge b_n = 1
   \end{align*}
   
   To model the step by step motion of an agent, the transition function is defined as the following:
   
   \RestyleAlgo{boxruled}
	\begin{algorithm}[H]
	\caption{Transition Function - Multi-goal }
		 \KwData{State, $\boldsymbol{x}$ }
		 \KwData{Action, $a$ }
		 \KwData{Maze Graph, $G_m$ }
		 \KwData{Goal Node Set, $V_g = \lbrace p_1, p_2, \cdots, p_n\rbrace$ }
		 \KwResult{New State, $\boldsymbol{x}_n$}
		 \;
		 
		 \tcp{Set the new state with the \\current state's boolean configuration}
		 \For{$k \in \lbrace 1, 2, \cdots, |V_g| \rbrace$}{
		 	$\boldsymbol{x}_n(b_k) = \boldsymbol{x}(b_k)$
		 }
		 \;
		 
		 \tcp{Update position in maze given the action $a$}
		 $\boldsymbol{x}_n(p) = \text{getNextPosition}\left(\boldsymbol{x},a,G_m\right)$
		 \;
		 \;
		 
		 \tcp{Check if new position is in goal node set
		 \\If so, find index associated with goal node and 
		 \\set corresponding boolean to $1$}
		 \If{ $\boldsymbol{x}_n(p) \in V_g$ }{
		 	Get $m \ni \boldsymbol{x}_n(p) = p_m$\;
		 	Set $\boldsymbol{x}_n(b_m) = 1$
		 }
		 \;
		 \Return $\boldsymbol{x}_n$
	\end{algorithm}
	
	Given the state representation and transition function described above, we are now able to implement and test our search-based agents. That said, we are now able to dive into solving each challenge of Part 1.
	
	\subsection{Part 1.1 - Basic Pathfinding}
   In this first challenge for Part 1, the goal is to implement search algorithms for navigating through a maze environment to a single goal location. For this challenge, we have implemented the following search algorithms:
   
   \begin{itemize}
   \item Depth-First Search
   \item Breadth-First Search
   \item Greedy Best-First Search
   \item A\textsuperscript{*} Search
   \end{itemize}
   
   For Greedy and A\textsuperscript{*}, we have also set their Heuristic Function as the Manhattan Distance metric, defined as the following:
   
   \begin{align*}
   d_M(u,v) = |u - v|_1
   \end{align*}
   
   where $u, v \in \mathbb{R}^2$ represent positions in the maze we want the distance between and $|\cdot|_1$ is the $L_1$ norm. Using this formula, the heuristic function will be:
   
   \begin{align*}
   h(n) = d_M(n,g)
\end{align*}      
   
   where $n$ is the 2-D position of the current agent's position and $g$ is the 2-D position of the goal point. Given the above search strategies and heuristic, Table \ref{tab:sol11} and Table \ref{tab:nnode11} show the stats between each method for each of the provided mazes:
   
   \begin{table}[ht]
   \centering
   \begin{tabular}{l | l | l | l  }
   \hline
    & Medium Maze & Big Maze & Open Maze\\
    \hline \hline 
   Depth-First Search & 124 & 474 & 59 \\
   Breadth-First Search & 68 & 148 & 45 \\
   Greedy Best-First Search & 68 & 222 & 77 \\
   A\textsuperscript{*} Search & 68 & 148 & 45 \\
   \hline
   \end{tabular}
   \caption{Solution Path Cost} \label{tab:sol11}
   \end{table}
   
   \begin{table}[ht]
   \centering
   \begin{tabular}{l | l | l | l  }
   \hline
    & Medium Maze & Big Maze & Open Maze\\
    \hline \hline 
   Depth-First Search & 198 & 1029 & 319 \\
   Breadth-First Search & 345 & 1259 & 523 \\
   Greedy Best-First Search & 77 & 311 & 27761 \\
   A\textsuperscript{*} Search & 202 & 1495 & 667 \\
   \hline
   \end{tabular}
   \caption{Number of Nodes Expanded} \label{tab:nnode11}
   \end{table}
   
   As we can see in the table, Depth-First Search (DFS) tended to arrive at a solution to a given maze quickly but at the expense of a sub-optimal path cost. We can note that Breadth First Search (BFS) and A* both, as expected, produced optimal path costs but it is interesting to note that for the Big Maze and Open Maze, BFS actually reached the optimal solution faster. It seems feasible that the Manhattan distance heuristic would, at times, take A* in a path that would lead to dead ends, in turn requiring A* to expand more nodes than BFS would have to. 
   
   One other thing to note was the interesting characteristics of the Greedy Search. We can see that on one end, it reached an optimal result in a very low number of expanded nodes for the Medium Maze. On the other end, it reaches a sub-optimal path cost after expanding a huge number of states in the Open Maze. Greedy obviously lacks robustness and this can be suspected due to the fact it does not take into account how far it has come but only thinks about how far away it thinks it is. Given this one sided thinking, the agent can get too excited being "close" to the goal even though they may be winding through a lot of poor sub-paths.
   
    \newpage
   \subsection{Part 1.2 - Search with Multiple Dots}
The goal of this part was to build heuristics for the A* algorithm that would allow an agent to tackle a set of multi-goal problems in a reasonable time frame. The heuristic being used for this section is one based on the Convex Hull. 

Let us define $p$ as the current location of the agent and define $V_u = \lbrace \hat{g}_1, \hat{g}_2, \cdots, \hat{g}_m \rbrace$ as the set of $m$ unvisited goal points. Let us then compute the convex hull of $\lbrace p \rbrace \bigcup V_u$, defined as:

\begin{align*}
\left(V_{ch}, E_{ch} \right) = \text{ConvexHull}\left(\lbrace p \rbrace \bigcup V_u \right)
\end{align*}

where $V_{ch}$ are the vertices on the convex hull of the input set and $E_{ch}$ is the edge set of 2-tuples such that $E_{ch} = \lbrace (x,y): x,y \text{ are ordered vertices of some edge} \rbrace$. For convenience, let us remove the largest edge touching point $p$, if any such edges exist, within the edge set $E_{ch}$. We will define this resulting set as $\hat{E}_{ch}$. With these results, the heuristic we choose is then defined as:

\begin{align*}
h_{ch}(p) = \sum_{(x,y) \in \hat{E}_{ch}} \beta d_{E}(x,y) + \left(1-\beta\right) d_{M}(x,y) + |V_u|
\end{align*}
   
   \newpage
   \subsection{Part 1 Extra Credit - Suboptimal Search}
   
   \newpage
   \section{Part 2 - Sokoban}
   Within this section, the goal is to develop an agent that is capable of solving the Sokoban puzzle problem for a set of input puzzles. To handle this problem, the state representation has been defined as the following:
   
   \begin{align*}
   \boldsymbol{x} = \left( p, b_1, b_2, \cdots, b_n \right)
   \end{align*}
   
   where in this case $p$ is the position of the agent, represented as a graph node index, and $b_k$ represents the position of the $k$\textsuperscript{th} box's position, represented as a graph node index, within the puzzle environment. Note that, as before, the input puzzle is processed to construct a graph representation which is in turn an implicit part of the state and environment representation for this problem. To know when the agent has found a Goal State, the following condition must be met:
   
   \begin{align*}
   \left(b_1 \in V_g \right) \wedge \left(b_2 \in V_g \right) \wedge \cdots \wedge \left(b_n \in V_g \right) = 1
   \end{align*}
   
   where $V_g = \lbrace g_1, g_2, \cdots, g_n\rbrace$ is the set of goal points within the puzzle we can place a box. The next step in formulating this problem is describing the Transition Model for the agent and what actions an agent can take based on its state. Algorithm \ref{algo:tmodel} describes the Transition Model for Sokoban given our state representation, while Algorithm \ref{algo:aset} refers to how we get the feasible action set for an agent given its current state. With these pieces described, we can readily approach solving Sokoban using the search methods discussed earlier.
   
   \RestyleAlgo{boxruled}
	\begin{algorithm} 
		 \KwData{State, $\boldsymbol{x}$ }
		 \KwData{Action, $a$, assumed to be valid }
		 \KwData{Maze Graph, $G_m$ }
		 \KwResult{New State, $\boldsymbol{x}_n$}
		 \;
		 
		 \tcp{Set the new state with the \\current state's box configuration}
		 \For{$k \in \lbrace 1, 2, \cdots, |V_g| \rbrace$}{
		 	$\boldsymbol{x}_n(b_k) = \boldsymbol{x}(b_k)$
		 }
		 \;
		 
		 \tcp{Update position in maze given the action $a$}
		 $\boldsymbol{x}_n(p) =  \text{getNextPosition}\left(\boldsymbol{x},a,G_m\right)$
		 \;
		 \;
		 
		 \tcp{Check if new position is in equivalent to\\
		 the position of one of the boxes. If so update\\
		 the location of the box using the same action}
		 \If{ $\boldsymbol{x}_n(p) \in \lbrace \boldsymbol{x}_n(b_1), \boldsymbol{x}_n(b_2), \cdots, \boldsymbol{x}_n(b_n) \rbrace$ }{
		 	Get $i \ni \boldsymbol{x}_n(p) = \boldsymbol{x}_n(b_i)$\;
		 	Set $\boldsymbol{x}_n(b_i) = \text{getNextPosition}\left(\boldsymbol{x}_n(b_i), a, G_m\right)$
		 }
		 \;
		 \Return $\boldsymbol{x}_n$
		 \caption{Sokoban Transition Model Algorithm}
		 \label{algo:tmodel}
	\end{algorithm}	
	
	\RestyleAlgo{boxruled}
	\begin{algorithm} 
		 \KwData{State, $\boldsymbol{x}$ }
		 \KwData{Maze Graph, $G_m$ }
		 \KwData{Goal Node Set, $V_g = \lbrace g_1, g_2, \cdots, g_n\rbrace$ }
		 \KwResult{Action Set, $A$}
		 \;
		 
		 \tcp{Get set of box positions}
		 $B_p = \text{getBoxPositions}\left(\boldsymbol{x}\right)$\;\;
		 
		 \tcp{Initialize Action Set as the one based on the Connectivity}
		 $A = \text{getActionsFromConnectivity}\left(\boldsymbol{x}(p), G_m\right)$ \;\;
		 
		 \tcp{Loop through possible actions and check they are valid\\
		 given the rules of Sokoban}
		 \For{$a \in A$}{
		 	
		 	\tcp{Get the next node in the graph the \\ agent would be going to}
		 	$p_n = \text{getNextPosition}\left(\boldsymbol{x},a,G_m\right)$\;\;
		 	
		 	\tcp{Check if $p_n$ is the position of a box}
		 	\If{$p_n \in B_p$}{
		 		\tcp{Update position of matching box using action $a$ \\and check if it is a valid position}
		 		$u =  \text{getNextPosition}\left(p_n,a,G_m\right)$\;\;
		 		
		 		\tcp{If new box position interferes with another box \\or the wall, remove $a$ from the action set $A$}
		 		\If{ $u \in B_p$ or $\text{isWall}\left(u,G_m\right)$ }{
		 			$A \leftarrow A \setminus \lbrace a\rbrace$
		 		}
		 	}
		 }
		 \;
		 
		 \tcp{Return the desired Action Set}
		 \Return $A$
		 
		 \caption{Sokoban Action Set Retrieval Algorithm}
		 \label{algo:aset}
	\end{algorithm}
   
\end{document}